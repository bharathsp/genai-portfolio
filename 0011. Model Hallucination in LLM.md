# Model hallucination

In Large Language Models (LLMs), **model hallucination** refers to the situation where the model generates text that is **syntactically correct and fluent but factually incorrect, fabricated, or nonsensical**.

It happens because LLMs don’t *truly know* facts — they predict the most likely sequence of words based on training data patterns. When they lack reliable information, they may "hallucinate" convincing but false content.

---

### 🔹 Causes of Hallucination

1. **Lack of knowledge** – The model hasn’t seen the required information in its training data.
2. **Overgeneralization** – The model mixes correct concepts but outputs wrong details.
3. **Prompt ambiguity** – If the prompt is vague, the model may fill gaps with invented content.
4. **Bias in training data** – If training data had errors, the model may replicate them.

---

### 🔹 Examples

1. **Fabricated Citations**
   *User:* “Give me references for research papers proving coffee cures cancer.”
   *Model:* “Sure! One such paper is *Smith et al., 2018, Journal of Medical Oncology, vol. 34(2), pp. 120-135*.”
   👉 The journal and paper don’t actually exist — the model made it up.

2. **Incorrect Facts**
   *User:* “Who was the first Indian astronaut to walk on the Moon?”
   *Model:* “Rakesh Sharma was the first Indian astronaut to walk on the Moon in 1984.”
   👉 False: Rakesh Sharma went to space but never walked on the Moon.

3. **Invented Product Features**
   *User:* “Does the iPhone 15 support holographic projection?”
   *Model:* “Yes, the iPhone 15 includes built-in holographic display technology.”
   👉 Made-up — Apple hasn’t released such a feature.

---

✅ In short: **Model hallucination = confident but false answers.**

---

## Here’s how companies and researchers try to **reduce hallucinations in LLMs**:

---

### 1. **Retrieval-Augmented Generation (RAG)**

* **Idea:** Instead of relying solely on the model’s memory, the model **queries external trusted sources** (databases, documents, APIs) for facts.
* **How it works:**

  1. User asks a question.
  2. The system retrieves relevant documents.
  3. The LLM generates an answer based on retrieved data.
* **Benefit:** Reduces making up facts since the model is grounded in actual sources.
* **Example:**

  * Without RAG: “The Eiffel Tower is 500m tall.” (hallucinated)
  * With RAG: “According to official records, the Eiffel Tower is 324m tall.”

---

### 2. **Fine-tuning with Fact-Checked Data**

* Models can be **fine-tuned on verified, high-quality datasets** (like Wikipedia, official documents).
* Focus on **factual correctness and consistency** rather than just fluency.
* **Benefit:** Reduces confidence in false statements.

---

### 3. **Prompt Engineering / Instruction Tuning**

* Giving the model **explicit instructions** to only answer what it knows or cite sources.
* Example prompt:

  > “Answer the question only if you are sure; otherwise say ‘I don’t know.’”
* **Benefit:** Reduces hallucination by discouraging guesses.

---

### 4. **Post-Generation Fact-Checking**

* After the LLM generates an answer, **automatic fact-checkers** verify the content.
* Can flag or correct hallucinations before showing them to the user.
* Example: Tools that cross-check claims against trusted knowledge graphs or databases.

---

### 5. **Chain-of-Thought & Self-Consistency**

* LLMs generate reasoning steps before giving an answer (chain-of-thought).
* By **verifying reasoning internally**, the model reduces inconsistent or fabricated conclusions.
* Example: Instead of saying directly “Rakesh Sharma walked on the Moon,” it first outlines “Rakesh Sharma went to space in 1984, Moon missions were Apollo…”, leading to the correct conclusion he didn’t walk on the Moon.

---

✅ **In short:** The key idea is to **ground LLM outputs in reality**—through external data, verified training, careful prompting, or automated verification.
